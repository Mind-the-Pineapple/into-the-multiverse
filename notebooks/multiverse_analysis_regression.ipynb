{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of multiverse_analysis cleaner try",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "multiverse",
      "language": "python",
      "name": "multiverse"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXZho2Mw5ako"
      },
      "source": [
        "This notebook can be run in two ways:\n",
        "- Run all cells from beginning to end. However, this is a time-consuming process that will take about 10hrs. Note that the maximal runtime of a colab notebook is 12hrs. If a section is time-consuming, then the estimated time will be reported at the beginning of the section. \n",
        "\n",
        "- Run the different sections independently. The necessary files are provided in the output folder and are saved as pickle at the end of each section. \n",
        "\n",
        "If not downloaded to the your machine all the new generated pickles will be lost once the colab session is terminated. \n",
        "If you want to download a file to your machine you can use:\n",
        "```python\n",
        "from google.colab import files\n",
        "files.download(str(output_path / 'file_name.p'))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHW7cYvz0a74"
      },
      "source": [
        "# 1. Setting up the enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzOXabS-wOg3"
      },
      "source": [
        "# Cloning the git repo with the data structure and complentary files\n",
        "# Note: to run the code in this notebook, you will have to accept that the \n",
        "# code was not developed by google.\n",
        "!git clone https://github.com/Mind-the-Pineapple/into-the-multiverse/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtCBYS1Q07ZK"
      },
      "source": [
        "# Install necessary python dependencies\n",
        "! pip install -r into-the-multiverse/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0PRP8tFe_su"
      },
      "source": [
        "Note: Remember to restart the runtime by clicking on the button above to have the same version of matplotlib, mpl_toolkits, numpy as specified in the requirement.txt file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1tU-46g1f72"
      },
      "source": [
        "## Download the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr9IDDki8O7P"
      },
      "source": [
        "All the data used for this project is [public availiable](https://figshare.com/articles/Data_for_Conservative_and_disruptive_modes_of_adolescent_change_in_human_brain_functional_connectivity_/11551602) and consists of 520 scans from 298 healthy [Váša et. al, 2020](https://www.pnas.org/content/117/6/3248) individuals (age 14-26, mean age = 19.24, see  for\n",
        "details)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiURCoijKCCM"
      },
      "source": [
        "from pathlib import Path\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "data_path = PROJECT_ROOT / 'into-the-multiverse' /'data' / 'age'\n",
        "output_path = PROJECT_ROOT / 'into-the-multiverse' / 'output'/ 'age'\n",
        "if not data_path.is_dir():\n",
        "    data_path.mkdir(parents=True)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVOQ5bQxoL-e"
      },
      "source": [
        "!wget -O into-the-multiverse/data/age/nspn.fmri.main.RData https://ndownloader.figshare.com/files/20958708"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16eWfSDM1uSU"
      },
      "source": [
        "!wget -O into-the-multiverse/data/age/nspn.fmri.gsr.RData https://ndownloader.figshare.com/files/20958699"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC9v-PY82lun"
      },
      "source": [
        "!wget -O into-the-multiverse/data/age/nspn.fmri.lowmot.RData https://ndownloader.figshare.com/files/20958702"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPaw6pyIhy4g"
      },
      "source": [
        "!wget -O into-the-multiverse/data/age/nspn.fmri.general.vars.RData https://ndownloader.figshare.com/files/20819796"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBm5C8HB3Okj"
      },
      "source": [
        "## Define key variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAHxS6LfG5ez"
      },
      "source": [
        "As mentioned this notebook was written so that every section could be run separately if needed. But in order to make this work, this section ([Define key variables](https://colab.research.google.com/drive/1fdEMsbZtQiTAwioeSn-JMLsJqcHqDoxj?authuser=2#)) needs to run and the variables that are going to be required saved into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65_d0CqVZowv"
      },
      "source": [
        "# Add the into-the-multiverse folder to the Python path. This allows the helperfunction\n",
        "# to be used\n",
        "\n",
        "import sys\n",
        "sys.path.insert(1, 'into-the-multiverse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra4NkHs2gjlH"
      },
      "source": [
        "import pickle\n",
        "import random\n",
        "\n",
        "import pyreadr \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colorbar\n",
        "import bct\n",
        "from scipy import stats\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "from helperfunctions import gateway_coef_sign, analysis_space\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6pjQl5L6Itv"
      },
      "source": [
        "def get_variables_of_interest():\n",
        "  # Set the random seed\n",
        "  #np.random.seed(2)\n",
        "  rng = np.random.default_rng(2)\n",
        "  random.seed(2)\n",
        "\n",
        "  # Define paths - REMOTE\n",
        "  PROJECT_ROOT = Path.cwd()\n",
        "  data_path = PROJECT_ROOT / 'into-the-multiverse' /'data' / 'age'\n",
        "  output_path = PROJECT_ROOT / 'into-the-multiverse' / 'output'/ 'age'\n",
        "\n",
        "\n",
        "  # Load data\n",
        "  data1 = pyreadr.read_r(str(data_path / 'nspn.fmri.main.RData'))\n",
        "  data3 = pyreadr.read_r(str(data_path / 'nspn.fmri.lowmot.RData'))\n",
        "  genVar = pyreadr.read_r(str(data_path / 'nspn.fmri.general.vars.RData'))\n",
        "  data2 = pyreadr.read_r(str(data_path / 'nspn.fmri.gsr.RData'))\n",
        "\n",
        "  DataNames=['nspn.fmri.main.RData','nspn.fmri.gsr.RData','nspn.fmri.lowmot.RData']\n",
        "\n",
        "  #Dictionary of 16 graph theory measures taken from the Brain Connectivity Toolbox\n",
        "  BCT_models = {\n",
        "    'degree': bct.degrees_und,\n",
        "    'strength': bct.strengths_und,\n",
        "    'betweennness centrality': bct.betweenness_bin,\n",
        "    'clustering (bin.)': bct.clustering_coef_bu,\n",
        "    'clustering (wei.)': bct.clustering_coef_wu,\n",
        "    'eigenvector centrality': bct.eigenvector_centrality_und,\n",
        "    'sugraph centrality': bct.subgraph_centrality,\n",
        "    'local efficiency' : bct.efficiency_bin,\n",
        "    'modularity (louvain)': bct.modularity_louvain_und,\n",
        "    'modularity (probtune)': bct.modularity_probtune_und_sign,\n",
        "    'participation coefficient': bct.participation_coef,\n",
        "    'module degree z-score': bct.module_degree_zscore,\n",
        "    'pagerank centrality': bct.pagerank_centrality,\n",
        "    'diversity coefficient': bct.diversity_coef_sign,\n",
        "    'gateway degree': gateway_coef_sign,\n",
        "    'k-core centrality': bct.kcoreness_centrality_bu,\n",
        "}\n",
        "  #Get info about brain regions and find Yeo network IDs; useful later on for \n",
        "  # graph metrics that need community labels.\n",
        "  KeptIDs = np.asarray(genVar['hcp.keep.id'])\n",
        "  YeoIDs = np.asarray(genVar['yeo.id.subc'])\n",
        "  KeptYeoIDs = YeoIDs[KeptIDs-1][:,0,0]\n",
        "\n",
        "  # Define some images properites\n",
        "  n_regions = 346\n",
        "  subject_array = 520\n",
        "\n",
        "  #Get motion regression functional connectivity data and reshape into \n",
        "  # region x region x subject array\n",
        "  FC = np.asarray(data1['fc.main'])\n",
        "  MainNoNan = np.nan_to_num(FC,copy=True,nan=1.0)\n",
        "  MainNoNanReshape = np.reshape(MainNoNan, [n_regions,n_regions,subject_array],\n",
        "                              order='F')\n",
        "\n",
        "  #Get global signal regression functional connectivity data and reshape into\n",
        "  # region x region x subject array\n",
        "  FC=np.asarray(data2['fc.gsr'])\n",
        "  GSRNoNan = np.nan_to_num(FC,copy=True,nan=1.0)\n",
        "  GSRNoNanReshape = np.reshape(GSRNoNan, [n_regions,n_regions,subject_array],\n",
        "                            order='F')\n",
        "\n",
        "  #Read in subject IDs and age\n",
        "  IDMain=np.asarray(data1['id.main'])\n",
        "  ages=np.asarray(data1['age.main'])\n",
        "\n",
        "  #Find unique subject IDs and index of first instance and find FC data \n",
        "  # corresponding to these indices\n",
        "  IDs,IDIndexUnique = np.unique(IDMain,return_index=True)\n",
        "  MainNoNanReshapeUnique = MainNoNanReshape[:,:,IDIndexUnique]\n",
        "  GSRNoNanReshapeUnique = GSRNoNanReshape[:,:,IDIndexUnique]\n",
        "  AgesUnique = ages[IDIndexUnique]\n",
        "  \n",
        "  # Number of randomly selected subjects to be used to define the low-dimensional \n",
        "  # space then split FC data and age data into two: 50 for defining space and \n",
        "  #remaining 248 for subsequent prediction\n",
        "  SpaceDefineIdx = 50\n",
        "  LockBoxDataIdx = 100\n",
        "  RandomIndexes = rng.choice(IDs.shape[0], size=IDs.shape[0], replace=False)\n",
        "  MainNoNanModelSpace = MainNoNanReshapeUnique[:,:,RandomIndexes[0:SpaceDefineIdx]]\n",
        "  MainNoNanLockBoxData = MainNoNanReshapeUnique[:, :, RandomIndexes[SpaceDefineIdx:LockBoxDataIdx]]\n",
        "  MainNoNanPrediction = MainNoNanReshapeUnique[:,:,RandomIndexes[LockBoxDataIdx:]]\n",
        "  \n",
        "  GSRNoNanModelSpace = GSRNoNanReshapeUnique[:,:,RandomIndexes[0:SpaceDefineIdx]]\n",
        "  GSRNoNanLockBoxData = GSRNoNanReshapeUnique[:,:,RandomIndexes[SpaceDefineIdx:LockBoxDataIdx]]\n",
        "  GSRNoNanPrediction = GSRNoNanReshapeUnique[:,:,RandomIndexes[LockBoxDataIdx:]]\n",
        "  \n",
        "  AgesModelSpace = AgesUnique[RandomIndexes[0:SpaceDefineIdx]]\n",
        "  AgesLockBoxData = AgesUnique[RandomIndexes[SpaceDefineIdx:LockBoxDataIdx]]\n",
        "  AgesPrediction = AgesUnique[RandomIndexes[LockBoxDataIdx:]]\n",
        "\n",
        "\n",
        "  return output_path, BCT_models, KeptYeoIDs, \\\n",
        "         AgesPrediction, MainNoNanPrediction, GSRNoNanPrediction, \\\n",
        "         AgesModelSpace, MainNoNanModelSpace, GSRNoNanModelSpace, \\\n",
        "         AgesLockBoxData, MainNoNanLockBoxData, GSRNoNanLockBoxData, \\\n",
        "         n_regions, subject_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Gv0TyC25o8"
      },
      "source": [
        "output_path, BCT_models, KeptYeoIDs, \\\n",
        "AgesPrediction, MainNoNanPrediction, GSRNoNanPrediction, \\\n",
        "AgesModelSpace, MainNoNanModelSpace, GSRNoNanModelSpace, \\\n",
        "AgesLockBoxData, MainNoNanLockBoxData, GSRNoNanLockBoxData, \\\n",
        "n_regions, subject_array = get_variables_of_interest()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbdR2G1WgRSM"
      },
      "source": [
        "Note: Some times running the cell above throws the following error:\n",
        "```\n",
        "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
        "```\n",
        "If this error shows up, restart the kernel and re-run all cells on this section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5mCueTin-12"
      },
      "source": [
        "# 2. Run the different analysis to bild the space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzK2ld8I-RYj"
      },
      "source": [
        "This section will perform the exhaustive evaluation of all 544 (2 different analysis, 17 sparsity thresholds and 16 nodal graph theoretical\n",
        "metrics) analysis approaches. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puxCTglyiKVF"
      },
      "source": [
        "BCT_Run = {}\n",
        "Sparsities_Run= {}\n",
        "Data_Run = {}\n",
        "GroupSummary = {}\n",
        "\n",
        "thresholds = [0.4,0.3,0.25,0.2,0.175,0.150,0.125,0.1,0.09,0.08,\n",
        "              0.07,0.06,0.05,0.04,0.03,0.02,0.01]\n",
        "preprocessings = ['MRS', 'GRS']\n",
        "\n",
        "n_thr = len(thresholds)\n",
        "n_pre = len(preprocessings)\n",
        "n_BCT = len(BCT_models.keys())\n",
        "Results = np.zeros(((n_thr * n_pre * n_BCT), n_regions))\n",
        "ResultsIndVar = np.zeros(((n_thr * n_pre * n_BCT), 1225))\n",
        "count=0\n",
        "with tqdm(range(n_thr * n_pre * n_BCT)) as pbar:\n",
        "  for pre_idx, DataPreproc in enumerate(preprocessings): # data preprocessing\n",
        "    if DataPreproc == 'MRS':\n",
        "        TempData = MainNoNanModelSpace\n",
        "        TotalSubjects = TempData.shape[2]\n",
        "    elif DataPreproc == 'GRS':\n",
        "        TempData = GSRNoNanModelSpace\n",
        "        TotalSubjects = TempData.shape[2]\n",
        "\n",
        "    for thr_idx, TempThreshold in enumerate(thresholds): # FC threshold level\n",
        "        for BCT_Num in BCT_models.keys(): # Graph theory measure\n",
        "            TempResults = np.zeros((TotalSubjects,n_regions))\n",
        "            for SubNum in range(TotalSubjects):\n",
        "                x = bct.threshold_proportional(TempData[:,:,SubNum],\n",
        "                                               TempThreshold, copy=True)\n",
        "                ss = analysis_space(BCT_Num, BCT_models, x, KeptYeoIDs)\n",
        "                #For each subject for each approach keep the 346 regional values.        \n",
        "                TempResults[SubNum, :] = ss \n",
        "\n",
        "            BCT_Run[count] = BCT_Num;\n",
        "            Sparsities_Run[count] = TempThreshold\n",
        "            Data_Run[count] = DataPreproc\n",
        "            GroupSummary[count] ='Mean'\n",
        "            # Build an array of similarities between subjects for each\n",
        "            # analysis approach \n",
        "            cos_sim = cosine_similarity(TempResults, TempResults)        \n",
        "            Results[count, :] = np.mean(TempResults, axis=0)\n",
        "            ResultsIndVar[count, :] = cos_sim[np.triu_indices(TotalSubjects, k=1)].T \n",
        "            count += 1\n",
        "            pbar.update(1)                        \n",
        "                     \n",
        "ModelsResults={\"Results\": Results,\n",
        "               \"ResultsIndVar\": ResultsIndVar,\n",
        "               \"BCT\": BCT_Run,\n",
        "               \"Sparsities\": Sparsities_Run, \n",
        "               \"Data\": Data_Run, \n",
        "               \"SummaryStat\": GroupSummary}\n",
        "            \n",
        "pickle.dump( ModelsResults, open(str(output_path / \"ModelsResults.p\"), \"wb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxVWimqHnyM0"
      },
      "source": [
        "# 3. Building and analysing the low-dimensional space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUYIJG675JWn"
      },
      "source": [
        "## Different embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkVxxGt8-Ci1"
      },
      "source": [
        "This section will use five different embedding algorithms to produce a low-dimension space that then be used for the active learning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ArZnzq35WAO"
      },
      "source": [
        "from sklearn import manifold, datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from time import time\n",
        "import pickle\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.ticker import NullFormatter\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.lines as mlines\n",
        "import matplotlib.pyplot as plt\n",
        "from umap.umap_ import UMAP\n",
        "import phate\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnseTiW87I53"
      },
      "source": [
        "# Load the previous results\n",
        "ModelResults = pickle.load(open(str(output_path / \"ModelsResults.p\"), \"rb\" ) )\n",
        "Results = ModelResults['ResultsIndVar']\n",
        "BCT_Run = ModelResults['BCT']\n",
        "Sparsities_Run = ModelResults['Sparsities']\n",
        "Data_Run = ModelResults['Data']\n",
        "preprocessings = ['MRS', 'GRS']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6sfdK1TiiVA"
      },
      "source": [
        "#Scale the data prior to dimensionality reduction\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(Results.T)\n",
        "X = X.T\n",
        "n_neighbors = 20\n",
        "n_components = 2 #number of components requested. In this case for a 2D space.\n",
        "\n",
        "#Define different dimensionality reduction techniques \n",
        "methods = OrderedDict()\n",
        "LLE = partial(manifold.LocallyLinearEmbedding,\n",
        "              n_neighbors, n_components, eigen_solver='dense')\n",
        "methods['LLE'] = LLE(method='standard', random_state=0)\n",
        "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
        "                                           n_neighbors=n_neighbors, random_state=0)\n",
        "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
        "                                 random_state=0)\n",
        "methods['UMAP'] = UMAP(random_state=40, n_components=2, n_neighbors=200,\n",
        "                             min_dist=.8)\n",
        "\n",
        "methods['PHATE'] = phate.PHATE()\n",
        "methods['PCA'] = PCA(n_components=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPvvEjch7Qju"
      },
      "source": [
        "markers = [\"x\",\"s\",\"o\",\"*\",\"D\",\"1\",\"v\",\"p\",\"H\",\"+\",\"|\",\"_\",\"3\",\"^\",\"4\",\"<\",\"X\"]\n",
        "colourmaps = {\"MRS\":\"Oranges\",\"GRS\":\"Purples\"}\n",
        "BCT = np.array(list(BCT_Run.items()))[:,1]\n",
        "Sparsities = np.array(list(Sparsities_Run.items()))[:,1]\n",
        "Data = np.array(list(Data_Run.items()))[:,1]\n",
        "\n",
        "# Reduced dimensions\n",
        "data_reduced = {}\n",
        "\n",
        "gsDE, axs = plt.subplots(3,2, figsize=(16,16), constrained_layout=True)\n",
        "axs = axs.ravel()\n",
        "#Perform embedding and plot the results (including info about the approach in the color/intensity and shape).\n",
        "\n",
        "for idx_method, (label, method) in enumerate(methods.items()):\n",
        "    Y = method.fit_transform(X)\n",
        "    # Save the results\n",
        "    data_reduced[label] = Y\n",
        "    Lines={}\n",
        "    for preprocessing in preprocessings:\n",
        "        BCTTemp=BCT[Data==preprocessing]\n",
        "        SparsitiesTemp=Sparsities[Data==preprocessing]\n",
        "        YTemp=Y[Data==preprocessing,:]\n",
        "\n",
        "        for idx_bct, bct_model in enumerate(BCT_models):\n",
        "            axs[idx_method].scatter(YTemp[:,0][BCTTemp==bct_model],\n",
        "                                    YTemp[:,1][BCTTemp==bct_model],\n",
        "                                    c=SparsitiesTemp[BCTTemp==bct_model], \n",
        "                                    marker=markers[idx_bct],\n",
        "                                    cmap=colourmaps[preprocessing], s=80)\n",
        "            Lines[idx_bct] = mlines.Line2D([], [], color='black', linestyle='None',\n",
        "                                marker=markers[idx_bct], markersize=10, \n",
        "                                label=bct_model)\n",
        "    # For visualisation purposes show the y and x labels only ons specific plots\n",
        "    if idx_method % 2 == 0: \n",
        "        axs[idx_method].set_ylabel('Dimension 1',fontsize=20)\n",
        "    if (idx_method == 4) or (idx_method == 5):\n",
        "        axs[idx_method].set_xlabel('Dimension 2',fontsize=20)\n",
        "\n",
        "    axs[idx_method].set_title(\"%s \" % (label),fontsize=20, fontweight=\"bold\")\n",
        "    axs[idx_method].axis('tight')\n",
        "    axs[idx_method].tick_params(labelsize=15)\n",
        "\n",
        "OrangePatch = mpatches.Patch(color='orange', label='Motion Regression')\n",
        "PurplePatch = mpatches.Patch(color='purple', label='Global Signal Regression')\n",
        "\n",
        "OrangePatch = mpatches.Patch(color='orange', label='motion regression')\n",
        "PurplePatch = mpatches.Patch(color=[85/255, 3/255, 152/255], label='global signal regression')\n",
        "\n",
        "IntensityPatch1 = mpatches.Patch(color=[0.1, 0.1, 0.1], label='threshold: 0.4',\n",
        "                                 alpha=1)\n",
        "IntensityPatch2 = mpatches.Patch(color=[0.1, 0.1, 0.1], label='threshold: 0.1', \n",
        "                                 alpha=0.4)\n",
        "IntensityPatch3 = mpatches.Patch(color=[0.1, 0.1, 0.1], label='threshold: 0.01', \n",
        "                                 alpha=0.1)\n",
        "\n",
        "BlankLine=mlines.Line2D([], [], linestyle='None')\n",
        "\n",
        "gsDE.legend(handles=[OrangePatch, PurplePatch,BlankLine,IntensityPatch1,\n",
        "                       IntensityPatch2, IntensityPatch3,BlankLine,\n",
        "                       Lines[0],Lines[1],Lines[2],Lines[3],Lines[4],Lines[5],\n",
        "                       Lines[6],Lines[7],Lines[8],Lines[9],Lines[10],Lines[11],\n",
        "                       Lines[12],Lines[13],Lines[14],Lines[15]],fontsize=15,\n",
        "              frameon=False,bbox_to_anchor=(1.25, .7))\n",
        "\n",
        "gsDE.savefig(str(output_path / 'DifferentEmbeddings.png'), dpi=300, bbox_inches='tight')\n",
        "gsDE.savefig(str(output_path / 'DifferentEmbeddings.svg'), format=\"svg\", bbox_inches='tight')\n",
        "gsDE.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52KI031j495K"
      },
      "source": [
        "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=10, \n",
        "                              random_state=21, metric=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggt7wfAfkAiD"
      },
      "source": [
        "#Do the same as above but for MDS\n",
        "Y = methods['MDS'].fit_transform(X)\n",
        "data_reduced['MDS'] = Y\n",
        "\n",
        "figMDS = plt.figure(constrained_layout=False, figsize=(21,15))\n",
        "gsMDS = figMDS.add_gridspec(nrows=15, ncols=20)\n",
        "axs = figMDS.add_subplot(gsMDS[:,0:15])\n",
        "idx_method = 0\n",
        "for preprocessing in preprocessings:\n",
        "    BCTTemp=BCT[Data==preprocessing]\n",
        "    SparsitiesTemp=Sparsities[Data==preprocessing]\n",
        "    YTemp=Y[Data==preprocessing,:]\n",
        "    Lines={}\n",
        "    \n",
        "    for idx_bct, bct_model in enumerate(BCT_models):\n",
        "        axs.scatter(YTemp[:,0][BCTTemp==bct_model],\n",
        "                                YTemp[:,1][BCTTemp==bct_model],\n",
        "                                c=SparsitiesTemp[BCTTemp==bct_model], \n",
        "                                marker=markers[idx_bct],\n",
        "                                norm=matplotlib.colors.Normalize(\n",
        "                                    vmin=np.min(SparsitiesTemp[BCTTemp==bct_model]),\n",
        "                                    vmax=np.max(SparsitiesTemp[BCTTemp==bct_model])),\n",
        "                                cmap=colourmaps[preprocessing], s=120)\n",
        "        Lines[idx_bct] = mlines.Line2D([], [], color='black', linestyle='None',\n",
        "                                marker=markers[idx_bct], markersize=10, \n",
        "                                label=bct_model)\n",
        "\n",
        "        axs.spines['top'].set_linewidth(1.5)\n",
        "        axs.spines['right'].set_linewidth(1.5)\n",
        "        axs.spines['bottom'].set_linewidth(1.5)\n",
        "        axs.spines['left'].set_linewidth(1.5)\n",
        "        axs.set_xlabel('Dimension 2',fontsize=20,fontweight=\"bold\")\n",
        "        axs.set_ylabel('Dimension 1',fontsize=20,fontweight=\"bold\")\n",
        "        axs.tick_params(labelsize=15)\n",
        "\n",
        "\n",
        "axs.set_title('Multi-dimensional Scaling', fontsize=25,fontweight=\"bold\")\n",
        "\n",
        "\n",
        "OrangePatch = mpatches.Patch(color='orange', label='motion regression')\n",
        "PurplePatch = mpatches.Patch(color=[85/255, 3/255, 152/255], label='global signal regression')\n",
        "\n",
        "IntensityPatch1 = mpatches.Patch(color=[0.1, 0.1, 0.1], label='threshold: 0.4',\n",
        "                                 alpha=1)\n",
        "IntensityPatch2 = mpatches.Patch(color=[0.1, 0.1, 0.1], label='threshold: 0.1', \n",
        "                                 alpha=0.4)\n",
        "IntensityPatch3 = mpatches.Patch(color=[0.1, 0.1, 0.1], label='threshold: 0.01', \n",
        "                                 alpha=0.1)\n",
        "\n",
        "BlankLine=mlines.Line2D([], [], linestyle='None')\n",
        "\n",
        "figMDS.legend(handles=[OrangePatch, PurplePatch,BlankLine,IntensityPatch1,\n",
        "                       IntensityPatch2, IntensityPatch3,BlankLine,\n",
        "                       Lines[0],Lines[1],Lines[2],Lines[3],Lines[4],Lines[5],\n",
        "                       Lines[6],Lines[7],Lines[8],Lines[9],Lines[10],Lines[11],\n",
        "                       Lines[12],Lines[13],Lines[14],Lines[15]],fontsize=15,\n",
        "              frameon=False,bbox_to_anchor=(1.4, 0.8),bbox_transform=axs.transAxes)\n",
        "\n",
        " \n",
        "figMDS.savefig(str(output_path / 'MDSSpace.png'), dpi=300)\n",
        "figMDS.savefig(str(output_path /'MDSSpace.svg'), format=\"svg\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TV38MzhNhkn"
      },
      "source": [
        "# Save results form the embedding to be used in the remaining analysis\n",
        "pickle.dump(data_reduced, open(str(output_path / \"embeddings.p\"), \"wb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y19DU5UH_EWC"
      },
      "source": [
        "## Analyse the neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x25diSb_IFr"
      },
      "source": [
        "from helperfunctions import (get_models_neighbours, get_dissimilarity_n_neighbours,\n",
        "                            get_null_distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOAQ8O518_cP"
      },
      "source": [
        "N = 544\n",
        "n_neighbors_step = 10\n",
        "\n",
        "neighbours_orig, adj_array = get_models_neighbours(N, n_neighbors_step, X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyfPmC_wNumB"
      },
      "source": [
        "neighbours_tsne, _ = get_models_neighbours(N, n_neighbors_step,\n",
        "                                           data_reduced['t-SNE'])\n",
        "diss_tsne = get_dissimilarity_n_neighbours(neighbours_orig, neighbours_tsne)\n",
        "del neighbours_tsne "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16BQTmevzoue"
      },
      "source": [
        "neighbours_lle, _ = get_models_neighbours(N, n_neighbors_step, \n",
        "                                          data_reduced['LLE'])\n",
        "diss_lle = get_dissimilarity_n_neighbours(neighbours_orig,neighbours_lle)\n",
        "del neighbours_lle "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phtJnBWCz1l6"
      },
      "source": [
        "neighbours_se, _ = get_models_neighbours(N, n_neighbors_step,\n",
        "                                         data_reduced['SE'])\n",
        "diss_se = get_dissimilarity_n_neighbours(neighbours_orig,neighbours_se)\n",
        "del neighbours_se"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r92E3fm3z6HC"
      },
      "source": [
        "neighbours_mds, _ = get_models_neighbours(N, n_neighbors_step,\n",
        "                                          data_reduced['MDS'])\n",
        "diss_mds = get_dissimilarity_n_neighbours(neighbours_orig,neighbours_mds)\n",
        "del neighbours_mds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpUpaN2YS5Nh"
      },
      "source": [
        "neighbours_pca, _ = get_models_neighbours(N, n_neighbors_step,\n",
        "                                          data_reduced['PCA'])\n",
        "diss_pca = get_dissimilarity_n_neighbours(neighbours_orig, neighbours_pca)\n",
        "del neighbours_pca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaxUNFasas6R"
      },
      "source": [
        "null_distribution = get_null_distribution(N, n_neighbors_step) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5XUNH1My_cW"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "n_neighbours = range(2, N, n_neighbors_step)\n",
        "ax.plot(n_neighbours, diss_tsne, label='t-SNE', color='#1DACE8')\n",
        "ax.plot(n_neighbours, diss_lle, label='LLE', color='#E5C4A1')\n",
        "ax.plot(n_neighbours, diss_se, label='SE', color='#F24D29')\n",
        "ax.plot(n_neighbours, diss_mds, label='MDS', color='#1C366B')\n",
        "ax.plot(n_neighbours, diss_pca, label='PCA', color='r')\n",
        "plt.plot(n_neighbours, null_distribution, label='random', c='grey')\n",
        "plt.ylim([0,1])\n",
        "plt.xlim([0,N])\n",
        "plt.legend(frameon=False)\n",
        "plt.xlabel('$k$ Nearest Neighbors')\n",
        "plt.ylabel('Dissimilarity $\\epsilon_k$')\n",
        "plt.savefig(str(output_path / 'dissimilarity_all.svg'))\n",
        "plt.savefig(str(output_path / 'dissimilarity_all.png'), dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4sodQ_wC4pY"
      },
      "source": [
        "# Download file to computer\n",
        "from google.colab import files\n",
        "files.download(str(output_path / 'dissimilarity_all.svg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8yM3HTB58LA"
      },
      "source": [
        "# 4. Exhaustive Search\n",
        "\n",
        "Exhaustive search for SVR prediction of age, so we know what \"ground truth\" is.\n",
        "\n",
        "Note: This step is time consuming and might take about 4hrs hrs to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QNBlUjI6Nwt"
      },
      "source": [
        "from bayes_opt import BayesianOptimization, UtilityFunction\n",
        "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from helperfunctions import objective_func_reg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRa95C7d8lVu"
      },
      "source": [
        "output_path, BCT_models, KeptYeoIDs, \\\n",
        "AgesPrediction, MainNoNanPrediction, GSRNoNanPrediction, \\\n",
        "AgesModelSpace, MainNoNanModelSpace, GSRNoNanModelSpace, \\\n",
        "AgesLockBoxData, MainNoNanLockBoxData, GSRNoNanLockBoxData, \\\n",
        "n_regions, subject_array = get_variables_of_interest()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLz4M0z4Ljj9"
      },
      "source": [
        "# Load embedding results. This cell is only necessary if you are running this\n",
        "# part of the analysis separatly.\n",
        "ModelEmbeddings = pickle.load(open(str(output_path / \"embeddings.p\"), \"rb\" ) )\n",
        "ModelEmbedding = ModelEmbeddings['MDS']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib3ZKhDMmPcU"
      },
      "source": [
        "PredictedAcc = np.zeros((len(Data_Run)))\n",
        "\n",
        "for i in tqdm(range(len(Data_Run))):\n",
        "    tempPredAcc = objective_func_reg(i, AgesPrediction, Sparsities_Run, Data_Run,\n",
        "                              BCT_models, BCT_Run, KeptYeoIDs, MainNoNanPrediction,\n",
        "                              GSRNoNanPrediction)\n",
        "    PredictedAcc[i] = tempPredAcc\n",
        "\n",
        "#Display how predicted accuracy is distributed across the low-dimensional space\n",
        "plt.scatter(ModelEmbedding[0: PredictedAcc.shape[0], 0],\n",
        "            ModelEmbedding[0: PredictedAcc.shape[0], 1],\n",
        "            c=PredictedAcc, cmap='bwr')\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aVnBYeLmQxa"
      },
      "source": [
        "# Dump accuracies\n",
        "pickle.dump(PredictedAcc, open(str(output_path / 'predictedAcc.pckl'), 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfOFZKFyCiJU"
      },
      "source": [
        "# Download file to computer\n",
        "from google.colab import files\n",
        "files.download(str(output_path / 'predictedAcc.pckl'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7UX0xYsKAdl"
      },
      "source": [
        "# 5. Active Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8aTNtvW3zp-"
      },
      "source": [
        "from itertools import product\n",
        "import pickle\n",
        "\n",
        "from matplotlib import cm\n",
        "import bct\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import permutation_test_score\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from helperfunctions import (initialize_bo, run_bo, posterior, \n",
        "                             posteriorOnlyModels, display_gp_mean_uncertainty,\n",
        "                             plot_bo_estimated_space, plot_bo_evolution,\n",
        "                             analysis_space, plot_bo_repetions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih9A4B4CIyYE"
      },
      "source": [
        "# Load embedding results. This cell is only necessary if you are running this\n",
        "# part of the analysis separatly.\n",
        "ModelEmbeddings = pickle.load(open(str(output_path / \"embeddings.p\"), \"rb\" ))\n",
        "ModelEmbedding = ModelEmbeddings['MDS']\n",
        "\n",
        "PredictedAcc = pickle.load(open(str(output_path / \"predictedAcc.pckl\"), \"rb\"))\n",
        "\n",
        "ModelResults = pickle.load(open(str(output_path / \"ModelsResults.p\"), \"rb\" ))\n",
        "Results = ModelResults['ResultsIndVar']\n",
        "BCT_Run = ModelResults['BCT']\n",
        "Sparsities_Run = ModelResults['Sparsities']\n",
        "Data_Run = ModelResults['Data']\n",
        "\n",
        "preprocessings = ['MRS', 'GRS']\n",
        "model_config = {}\n",
        "model_config['Sparsities_Run'] = Sparsities_Run\n",
        "model_config['Data_Run'] = Data_Run\n",
        "model_config['BCT_models'] = BCT_models\n",
        "model_config['BCT_Run'] = BCT_Run\n",
        "model_config['CommunityIDs'] = KeptYeoIDs\n",
        "model_config['MainNoNanPrediction'] = MainNoNanPrediction\n",
        "model_config['GSRNoNanPrediction'] = GSRNoNanPrediction\n",
        "model_config['MainNoNanLockBox'] = MainNoNanLockBoxData\n",
        "model_config['GSRNoNanLockBox'] = GSRNoNanLockBoxData\n",
        "ClassOrRegression = 'Regression'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3pdTLOC_WU4"
      },
      "source": [
        "## Exploratory analysis\n",
        "\n",
        "Note: This step takes about 30min."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l81tOZ9Z33WD"
      },
      "source": [
        "kappa = 10\n",
        "\n",
        "# Define settings for the analysis\n",
        "kernel, optimizer, utility, init_points, n_iter, pbounds, nbrs, RandomSeed = \\\n",
        "                      initialize_bo(ModelEmbedding, kappa)\n",
        "\n",
        "# Perform optimization. Given that the space is continuous and the analysis \n",
        "# approaches are not, we penalize suggestions that are far from any actual \n",
        "# analysis approaches. For these suggestions the registered value is set to the\n",
        "#  lowest value from the burn in. These points (BadIters) are only used\n",
        "# during search but exluded when recalculating the GP regression after search.\n",
        "BadIter = run_bo(optimizer, utility, init_points,\n",
        "                 n_iter, pbounds, nbrs, RandomSeed,\n",
        "                 ModelEmbedding, model_config, \n",
        "                 AgesPrediction,\n",
        "                 ClassOrRegression,\n",
        "                 MultivariateUnivariate=True, verbose=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSUlVAr3LQa2"
      },
      "source": [
        "x_exploratory, y_exploratory, z_exploratory, x, y, gp, vmax, vmin = \\\n",
        "                                           plot_bo_estimated_space(kappa, BadIter,\n",
        "                                              optimizer, pbounds, \n",
        "                                              ModelEmbedding, PredictedAcc, \n",
        "                                              kernel, output_path, ClassOrRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQOakRcjLTCa"
      },
      "source": [
        "# Display the results of the active search and the evolution of the search\n",
        "# after 5, 10,20, 30 and 50 iterations.\n",
        "corr = plot_bo_evolution(kappa, x_exploratory, y_exploratory, z_exploratory, x, y, gp,\n",
        "                  vmax, vmin, ModelEmbedding, PredictedAcc, output_path, ClassOrRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLDCqGX2eVVA"
      },
      "source": [
        "print(f'Spearman correlation {corr}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gchNrIvexHA"
      },
      "source": [
        "## Exploitatory analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVoIxA3173sM"
      },
      "source": [
        "kappa = .1\n",
        "\n",
        "# Define settins for the analysis\n",
        "kernel, optimizer, utility, init_points, n_iter, pbounds, nbrs, RandomSeed = \\\n",
        "                      initialize_bo(ModelEmbedding, kappa)\n",
        "\n",
        "# Perform optimization. Given that the space is continuous and the analysis \n",
        "# approaches are not, we penalize suggestions that are far from any actual \n",
        "# analysis approaches. For these suggestions the registered value is set to the\n",
        "#  lowest value from the burn in. These points (BadIters) are only used\n",
        "# during search but exluded when recalculating the GP regression after search.\n",
        "BadIter = run_bo(optimizer, utility, init_points,\n",
        "                 n_iter, pbounds, nbrs, RandomSeed,\n",
        "                 ModelEmbedding, model_config, \n",
        "                 AgesPrediction,\n",
        "                 ClassOrRegression,\n",
        "                 MultivariateUnivariate=True, verbose=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rijtkCYjfGa7"
      },
      "source": [
        "x_exploratory, y_exploratory, z_exploratory, x, y, gp, vmax, vmin = \\\n",
        "                                           plot_bo_estimated_space(kappa, BadIter,\n",
        "                                              optimizer, pbounds, \n",
        "                                              ModelEmbedding, PredictedAcc, \n",
        "                                              kernel, output_path, ClassOrRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7DbkSuNfJjh"
      },
      "source": [
        "# Display the results of the active search and the evolution of the search\n",
        "# after 5, 10,20, 30 and 50 iterations.\n",
        "plot_bo_evolution(kappa, x_exploratory, y_exploratory, z_exploratory, x, y, gp,\n",
        "                  vmax, vmin, ModelEmbedding, PredictedAcc, output_path, ClassOrRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyc9XH6ufdzV"
      },
      "source": [
        "# Download file to computer\n",
        "from google.colab import files\n",
        "files.download(str(output_path / 'BOptEvolutionK10.svg'))\n",
        "files.download(str(output_path / 'BOptEvolutionK0.1.svg'))\n",
        "files.download(str(output_path / 'BOptAndTrueK0.1.svg'))\n",
        "files.download(str(output_path / 'BOptAndTrueK10.svg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8prbJVG3Z6M"
      },
      "source": [
        "## Repetitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsttJ2FeDsac"
      },
      "source": [
        "This is time consuming step and will take about 4 hrs to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCGuS3mFDNmA"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "\n",
        "n_repetitions = 20\n",
        "kappa = 10\n",
        "TotalRegions = 346\n",
        "n_permutations = 1000\n",
        "\n",
        "BestModelGPSpace = np.zeros(n_repetitions)\n",
        "BestModelGPSpaceModIndex = np.zeros(n_repetitions)\n",
        "BestModelEmpirical = np.zeros(n_repetitions)\n",
        "BestModelEmpiricalModIndex = np.zeros(n_repetitions)\n",
        "ModelActualAccuracyCorrelation = np.zeros(n_repetitions)\n",
        "CVPValBestModels = np.zeros(n_repetitions)\n",
        "perm_scores = np.zeros((n_repetitions, n_permutations))\n",
        "cv_mae = np.zeros(n_repetitions)\n",
        "maes = np.zeros(n_repetitions)\n",
        "#predictions = np.zeros((n_repetitions, len(AgesLockBoxData)))\n",
        "\n",
        "for DiffInit in range(n_repetitions):\n",
        "    # Define settings for the analysis\n",
        "    kernel, optimizer, utility, init_points, n_iter, pbounds, nbrs, RandomSeed = \\\n",
        "                      initialize_bo(ModelEmbedding, kappa, repetitions=True,\n",
        "                                    DiffInit=DiffInit)\n",
        "    \n",
        "    # Run BO on the Prediction again\n",
        "    FailedIters = run_bo(optimizer, utility, init_points,\n",
        "                         n_iter, pbounds, nbrs, RandomSeed,\n",
        "                         ModelEmbedding, model_config, \n",
        "                         AgesPrediction,\n",
        "                         ClassOrRegression,\n",
        "                         MultivariateUnivariate=True, repetitions=True,\n",
        "                         verbose=False)\n",
        "    gp = GaussianProcessRegressor(kernel=kernel, normalize_y=True,\n",
        "                                  n_restarts_optimizer=10)\n",
        "\n",
        "    x_temp = np.array([[res[\"params\"][\"b1\"]] for res in optimizer.res])\n",
        "    y_temp = np.array([[res[\"params\"][\"b2\"]] for res in optimizer.res])\n",
        "    z_temp = np.array([res[\"target\"] for res in optimizer.res])\n",
        "\n",
        "    x_obs = x_temp[FailedIters==0]\n",
        "    y_obs = y_temp[FailedIters==0]\n",
        "    z_obs = z_temp[FailedIters==0]\n",
        "    \n",
        "    muModEmb, sigmaModEmb, gpModEmb = posteriorOnlyModels(gp, x_obs, y_obs, z_obs,\n",
        "                                                      ModelEmbedding)\n",
        "\n",
        "    BestModelGPSpace[DiffInit] = muModEmb.max()\n",
        "    BestModelGPSpaceModIndex[DiffInit] = muModEmb.argmax()\n",
        "    BestModelEmpirical[DiffInit] = z_obs.max()\n",
        "    Model_coord = np.array([[x_obs[z_obs.argmax()][-1], y_obs[z_obs.argmax()][-1]]])\n",
        "    BestModelEmpiricalModIndex[DiffInit] = nbrs.kneighbors(Model_coord)[1][0][0]\n",
        "    ModelActualAccuracyCorrelation[DiffInit] = spearmanr(muModEmb, PredictedAcc)[0]\n",
        "    \n",
        "    TempModelNum = muModEmb.argmax()\n",
        "    TempThreshold = Sparsities_Run[TempModelNum]\n",
        "    BCT_Num = BCT_Run[TempModelNum]\n",
        "\n",
        "    # Load the Lockbox data\n",
        "    Y = AgesLockBoxData\n",
        "    CommunityIDs = KeptYeoIDs\n",
        "    if Data_Run[TempModelNum] == 'MRS':\n",
        "        TempDataLockBox = MainNoNanLockBoxData\n",
        "        TempDataPredictions = MainNoNanPrediction\n",
        "\n",
        "    elif Data_Run[TempModelNum] == 'GRS':\n",
        "        TempDataLockBox = GSRNoNanLockBoxData\n",
        "        TempDataPredictions = MainNoNanPrediction\n",
        "\n",
        "    TotalSubjectslock = TempDataLockBox.shape[2]\n",
        "    TotalSubjectsPredictions = TempDataPredictions.shape[2]   \n",
        "\n",
        "    TempResultsLockData = np.zeros([TotalSubjectslock, n_regions])\n",
        "    for SubNum in range(0, TotalSubjectslock):\n",
        "        # Lock data\n",
        "        x = bct.threshold_proportional(TempDataLockBox[:, :, SubNum],\n",
        "                                        TempThreshold, copy=True)\n",
        "        ss = analysis_space(BCT_Num, BCT_models, x, KeptYeoIDs)\n",
        "        TempResultsLockData[SubNum, :] = ss \n",
        "\n",
        "    TempPredictionsData = np.zeros([TotalSubjectsPredictions, n_regions])\n",
        "    for SubNum in range(0, TotalSubjectsPredictions):\n",
        "        # Lock data\n",
        "        x = bct.threshold_proportional(TempDataPredictions[:, :, SubNum],\n",
        "                                        TempThreshold, copy=True)\n",
        "        ss = analysis_space(BCT_Num, BCT_models, x, KeptYeoIDs)\n",
        "        TempPredictionsData[SubNum, :] = ss \n",
        "\n",
        "    model = Pipeline([('scaler', StandardScaler()), ('svr', SVR())])\n",
        "    all_data = np.concatenate((TempPredictionsData, TempResultsLockData))\n",
        "    test_fold = np.concatenate((- np.ones(len(TempPredictionsData)),np.zeros(len(TempResultsLockData))))\n",
        "    all_ages = np.concatenate((AgesPrediction.ravel(), AgesLockBoxData.ravel()))\n",
        "    ps = PredefinedSplit(test_fold)\n",
        "    mae, perm_score, p_val = permutation_test_score(model, all_data, all_ages,\n",
        "                                  n_jobs=None, random_state=5, verbose=0,\n",
        "                                  groups=None, cv=ps, n_permutations=n_permutations, \n",
        "                                  scoring=\"neg_mean_absolute_error\")\n",
        "    cv_mae[DiffInit] = mae\n",
        "    CVPValBestModels[DiffInit] = p_val\n",
        "    perm_scores[DiffInit, :] = perm_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMqeSsZlL28V"
      },
      "source": [
        "  plot_bo_repetions(ModelEmbedding, PredictedAcc, BestModelGPSpaceModIndex,\n",
        "                  BestModelEmpiricalModIndex, BestModelEmpirical,\n",
        "                  ModelActualAccuracyCorrelation, output_path, ClassOrRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU9lX346z3h1"
      },
      "source": [
        "# Download image to computer\n",
        "from google.colab import files\n",
        "files.download(str(output_path / 'BOpt20Repeats.svg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpuIeYBvKN0X"
      },
      "source": [
        "import pandas as pd\n",
        "# Obtain the list of 20 models that were defined as the best models\n",
        "df = pd.DataFrame({'Data_Run': Data_Run,'sparsities': Sparsities_Run, \n",
        "                   'bct': BCT_Run})\n",
        "df_best = df.iloc[BestModelEmpiricalModIndex]\n",
        "df_best['mae']= cv_mae \n",
        "df_best['p-val'] = CVPValBestModels\n",
        "df_best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kXvhDOM--CW"
      },
      "source": [
        "repetions_results = {\n",
        "                      'dataframe': df_best,\n",
        "                      'BestModelGPSpaceModIndex': BestModelGPSpaceModIndex,\n",
        "                      'BestModelEmpiricalIndex':  BestModelEmpiricalModIndex, \n",
        "                      'BestModelEmpirical': BestModelEmpirical,\n",
        "                      'ModelActualAccuracyCorrelation': ModelActualAccuracyCorrelation\n",
        "                     }\n",
        "pickle.dump( repetions_results, open(str(output_path / \"repetitions_results.p\"), \"wb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjXx0Fn2-3A-"
      },
      "source": [
        "print(df_best.to_latex(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZklq7xAOx2Q"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(str(output_path / 'repetitions_results.p'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIqSVje1Nq68"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}