{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiverse_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHW7cYvz0a74",
        "colab_type": "text"
      },
      "source": [
        "# Setting up the enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzOXabS-wOg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cloning the git repo with the data structure\n",
        "!git clone https://github.com/JessyD/test.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtCBYS1Q07ZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install necessary python dependencies\n",
        "! pip install -r test/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1tU-46g1f72",
        "colab_type": "text"
      },
      "source": [
        "# Download the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVOQ5bQxoL-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O test/data/nspn.fmri.main.RData https://ndownloader.figshare.com/files/20958708"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16eWfSDM1uSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O test/data/nspn.fmri.gsr.RData https://ndownloader.figshare.com/files/20958699"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC9v-PY82lun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O test/data/nspn.fmri.lowmot.RData https://ndownloader.figshare.com/files/20958702"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPaw6pyIhy4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O test/data/nspn.fmri.general.vars.RData https://ndownloader.figshare.com/files/20819796"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBm5C8HB3Okj",
        "colab_type": "text"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra4NkHs2gjlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import random\n",
        "\n",
        "import pyreadr \n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colorbar\n",
        "import bct\n",
        "from scipy import stats\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQR-qpAVhTz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the random seed\n",
        "#np.random.seed(2)\n",
        "rng = default_rng(2)\n",
        "random.seed(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mapfQZqEixXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define paths\n",
        "from pathlib import Path\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "data_path = PROJECT_ROOT / 'test' /'data'\n",
        "output_path = PROJECT_ROOT / 'test' / 'output'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he7t6Vu6omM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data1 = pyreadr.read_r(str(data_path / 'nspn.fmri.main.RData'))\n",
        "genVar = pyreadr.read_r(str(data_path / 'nspn.fmri.general.vars.RData'))\n",
        "#data3 = pyreadr.read_r(str(data_path / 'nspn.fmri.lowmot.RData'))\n",
        "data2 = pyreadr.read_r(str(data_path / 'nspn.fmri.gsr.RData'))\n",
        "\n",
        "DataNames=['nspn.fmri.main.RData','nspn.fmri.gsr.RData','nspn.fmri.lowmot.RData']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Gv0TyC25o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define some images properites\n",
        "n_regions = 346\n",
        "subject_array = 520\n",
        "\n",
        "#Get motion regression functional connectivity data and reshape into \n",
        "# region x region x subject array\n",
        "FC = np.asarray(data1['fc.main'])\n",
        "MainNoNan = np.nan_to_num(FC,copy=True,nan=1.0)\n",
        "MainNoNanReshape = np.reshape(MainNoNan, [n_regions,n_regions,subject_array],\n",
        "                            order='F')\n",
        "\n",
        "#Get global signal regression functional connectivity data and reshape into\n",
        "# region x region x subject array\n",
        "FC=np.asarray(data2['fc.gsr'])\n",
        "GSRNoNan = np.nan_to_num(FC,copy=True,nan=1.0)\n",
        "GSRNoNanReshape = np.reshape(GSRNoNan, [n_regions,n_regions,subject_array],\n",
        "                           order='F')\n",
        "\n",
        "#Read in subject IDs and age\n",
        "IDMain=np.asarray(data1['id.main'])\n",
        "ages=np.asarray(data1['age.main'])\n",
        "\n",
        "#Find unique subject IDs and index of first instance and find FC data \n",
        "# corresponding to these indices\n",
        "IDs,IDIndexUnique = np.unique(IDMain,return_index=True)\n",
        "MainNoNanReshapeUnique = MainNoNanReshape[:,:,IDIndexUnique]\n",
        "GSRNoNanReshapeUnique = GSRNoNanReshape[:,:,IDIndexUnique]\n",
        "AgesUnique = ages[IDIndexUnique]\n",
        "\n",
        "# Number of randomly selected subjects to be used to define the low-dimensional \n",
        "# space then split FC data and age data into two: 50 for defining space and \n",
        "#remaining 248 for subsequent prediction\n",
        "SpaceDefineN = 50\n",
        "RandomIndexes = rng.choice(IDs.shape[0], size=IDs.shape[0], replace=False)\n",
        "MainNoNanModelSpace = MainNoNanReshapeUnique[:,:,RandomIndexes[0:SpaceDefineN]]\n",
        "MainNoNanPrediction = MainNoNanReshapeUnique[:,:,RandomIndexes[SpaceDefineN:]]\n",
        "GSRNoNanModelSpace = GSRNoNanReshapeUnique[:,:,RandomIndexes[0:SpaceDefineN]]\n",
        "GSRNoNanPrediction = GSRNoNanReshapeUnique[:,:,RandomIndexes[SpaceDefineN:]]\n",
        "AgesModelSpace = AgesUnique[RandomIndexes[0:SpaceDefineN]]\n",
        "AgesPrediction = AgesUnique[RandomIndexes[SpaceDefineN:]]\n",
        "IDsModelSpace = IDs[RandomIndexes[0:SpaceDefineN]] \n",
        "IDsPrediction = IDs[RandomIndexes[SpaceDefineN:]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DAI4LaEsydR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get info about brain regions and find Yeo network IDs; useful later on for \n",
        "# graph metrics that need community labels.\n",
        "KeptIDs = np.asarray(genVar['hcp.keep.id'])\n",
        "YeoIDs = np.asarray(genVar['yeo.id.subc'])\n",
        "KeptYeoIDs = YeoIDs[KeptIDs-1][:,0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79k8wv4MhhnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dictionary of 16 graph theory measures taken from the Brain Connectivity Toolbox\n",
        "\n",
        "BCT_models = {\n",
        "    'degree': bct.degrees_und,\n",
        "    'strength': bct.strengths_und,\n",
        "    'betweennness centrality': bct.betweenness_bin,\n",
        "    'clustering (bin.)': bct.clustering_coef_bu,\n",
        "    'clustering (wei.)': bct.clustering_coef_wu,\n",
        "    'eigenvector centrality': bct.eigenvector_centrality_und,\n",
        "    'sugraph centrality': bct.subgraph_centrality,\n",
        "    'local efficiency' : bct.efficiency_bin,\n",
        "    'modularity (louvain)': bct.modularity_louvain_und,\n",
        "    'modularity (probtune)': bct.modularity_probtune_und_sign,\n",
        "    'participation coefficient': bct.participation_coef,\n",
        "    'module degree z-score': bct.module_degree_zscore,\n",
        "    'pagerank centrality': bct.pagerank_centrality,\n",
        "    'diversity coefficient': bct.diversity_coef_sign,\n",
        "    'gateway degree': bct.gateway_coef_sign,\n",
        "    'k-core centrality': bct.kcoreness_centrality_bu,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5mCueTin-12",
        "colab_type": "text"
      },
      "source": [
        "## Generating data to build low-dimensional space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puxCTglyiKVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This involves exhaustive evaluation of all 544 analysis approaches.  \n",
        "\n",
        "BCT_Run={}\n",
        "Sparsities_Run= {}\n",
        "Data_Run = {}\n",
        "GroupSummary = {}\n",
        "\n",
        "thresholds = [0.4,0.3,0.25,0.2,0.175,0.150,0.125,0.1,0.09,0.08,\n",
        "              0.07,0.06,0.05,0.04,0.03,0.02,0.01]\n",
        "preprocessing = ['MRS', 'GRS']\n",
        "\n",
        "n_thr = len(thresholds)\n",
        "n_pre = len(preprocessing)\n",
        "n_BCT = len(BCT_models.keys())\n",
        "Results = np.zeros(((n_thr * n_pre * n_BCT), n_regions))\n",
        "ResultsIndVar = np.zeros(((n_thr * n_pre * n_BCT), 1225))\n",
        "count=0\n",
        "for DataPreproc in preprocessing: # data preprocessing\n",
        "    if DataPreproc == 'MRS':\n",
        "        TempData = MainNoNanModelSpace\n",
        "        TotalSubjects = TempData.shape[2]\n",
        "    elif DataPreproc == 'GRS':\n",
        "        TempData = GSRNoNanModelSpace\n",
        "        TotalSubjects = TempData.shape[2]\n",
        "\n",
        "    for thr_idx, TempThreshold in enumerate(thresholds): # FC threshold level\n",
        "        for BCT_Num in BCT_models.keys(): # Graph theory measure\n",
        "            TempResults = np.zeros((TotalSubjects,n_regions))\n",
        "            for SubNum in range(TotalSubjects):\n",
        "                x = bct.threshold_proportional(TempData[:, :, SubNum], \n",
        "                                               TempThreshold, copy=True)\n",
        "                if BCT_Num == 'local efficiency':\n",
        "                    ss = BCT_models[BCT_Num](x,1)\n",
        "                elif BCT_Num == 'modularity (louvain)':\n",
        "                    temp = BCT_models[BCT_Num](x, seed=2)\n",
        "                    ss = temp[0]\n",
        "                elif BCT_Num== 'modularity (probtune)':\n",
        "                    temp = BCT_models[BCT_Num](x, seed=2)\n",
        "                    ss = temp[0]\n",
        "                elif BCT_Num == 'participation coefficient':\n",
        "                    ss = BCT_models[BCT_Num](x, KeptYeoIDs)\n",
        "                elif BCT_Num == 'module degree z-score':\n",
        "                    ss = BCT_models[BCT_Num](x, KeptYeoIDs)\n",
        "                elif BCT_Num == 'pagerank centrality':\n",
        "                    ss = BCT_models[BCT_Num](x, 0.85)\n",
        "                elif BCT_Num == 'diversity coefficient':\n",
        "                    temp = BCT_models[BCT_Num](x, KeptYeoIDs)\n",
        "                    ss = temp[0]\n",
        "                elif BCT_Num == 'gateway degree':\n",
        "                    temp = BCT_models[BCT_Num](x, KeptYeoIDs)\n",
        "                    ss = temp[0]\n",
        "                elif BCT_Num == 'k-core centrality':\n",
        "                    temp = BCT_models[BCT_Num](x)\n",
        "                    ss = temp[0]\n",
        "                else:\n",
        "                    ss = BCT_models[BCT_Num](x)\n",
        "                #For each subject for each approach keep the 346 regional values.        \n",
        "                TempResults[SubNum, :] = ss \n",
        "\n",
        "            BCT_Run[count] = BCT_Num;\n",
        "            Sparsities_Run[count] = TempThreshold\n",
        "            Data_Run[count] = DataPreproc\n",
        "            GroupSummary[count] ='Mean'\n",
        "            # Build an array of similarities between subjects for each\n",
        "            # analysis approach \n",
        "            cos_sim = cosine_similarity(TempResults, TempResults)        \n",
        "            Results[count, :] = np.mean(TempResults, axis=0)\n",
        "            ResultsIndVar[count, :] = cos_sim[np.triu_indices(TotalSubjects, k=1)].T                         \n",
        "            count = count + 1\n",
        "                     \n",
        "ModelsResults={\"Results\": Results,\n",
        "               \"ResultsIndVar\": ResultsIndVar,\n",
        "               \"BCT\": BCT_Run,\n",
        "               \"Sparsities\": Sparsities_Run, \n",
        "               \"Data\": Data_Run, \n",
        "               \"SummaryStat\": GroupSummary}\n",
        "            \n",
        "pickle.dump( ModelsResults, open(str(output_path / \"ModelsResults.p\"), \"wb\" ) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxVWimqHnyM0",
        "colab_type": "text"
      },
      "source": [
        "## Building the low-dimensional space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUYIJG675JWn",
        "colab_type": "text"
      },
      "source": [
        "### LLE, SE, tSNE Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ArZnzq35WAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import manifold, datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from time import time\n",
        "import pickle\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.ticker import NullFormatter\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.lines as mlines\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6sfdK1TiiVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the previous results\n",
        "ModelResults = pickle.load(open(str(output_path / \"ModelsResults.p\"), \"rb\" ) )\n",
        "Results = ModelResults['ResultsIndVar']\n",
        "BCT_Run = ModelResults['BCT']\n",
        "Sparsities_Run = ModelResults['Sparsities']\n",
        "Data_Run = ModelResults['Data']\n",
        "GroupSummary = ModelResults['SummaryStat']\n",
        "Ages = np.asarray(data1['age.main'])\n",
        "\n",
        "#Scale the data prior to dimensionality reduction\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(Results.T)\n",
        "X = X.T\n",
        "n_neighbors = 20\n",
        "n_components = 2 #number of components requested. In this case for a 2D space.\n",
        "\n",
        "#Define different dimensionality reduction techniques \n",
        "methods = OrderedDict()\n",
        "LLE = partial(manifold.LocallyLinearEmbedding,\n",
        "              n_neighbors, n_components, eigen_solver='dense')\n",
        "methods['LLE'] = LLE(method='standard', random_state=0)\n",
        "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
        "                                           n_neighbors=n_neighbors, random_state=0)\n",
        "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
        "                                 random_state=0)\n",
        "\n",
        "\n",
        "markers = [\"x\",\"s\",\"o\",\"*\",\"D\",\"1\",\"v\",\"p\",\"H\",\"+\",\"|\",\"_\",\"3\",\"^\",\"4\",\"<\",\"X\"]\n",
        "colourmaps = {\"MRS\":\"Oranges\",\"GRS\":\"Purples\"}\n",
        "BCT = np.array(list(BCT_Run.items()))[:,1]\n",
        "Sparsities = np.array(list(Sparsities_Run.items()))[:,1]\n",
        "Data=np.array(list(Data_Run.items()))[:,1]\n",
        "\n",
        "# Reduced dimensions\n",
        "data_reduced = {}\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "fig.subplots_adjust(right=0.7)\n",
        "figDE = plt.figure(constrained_layout=False, figsize=(21,6))\n",
        "gsDE = figDE.add_gridspec(nrows=6, ncols=21)#, left=0.05, right=0.48, wspace=0.05)\n",
        "\n",
        "#Perform embedding and plot the results (including info about the approach in the color/intensity and shape).\n",
        "\n",
        "for i, (label, method) in enumerate(methods.items()):\n",
        "     \n",
        "    t0 = time()\n",
        "    Y = method.fit_transform(X)\n",
        "\n",
        "    t1 = time()\n",
        "    # Save the results\n",
        "    data_reduced[label] = Y\n",
        "    \n",
        "    ax = figDE.add_subplot(gsDE[:,i*6+i:(i+1)*6+i])\n",
        "    for d in preprocessing:\n",
        "\n",
        "        BCTTemp=BCT[Data==d]\n",
        "        SparsitiesTemp=Sparsities[Data==d]\n",
        "        YTemp=Y[Data==d,:]\n",
        "\n",
        "        \n",
        "        for i, c in enumerate(np.unique(BCTTemp)):\n",
        "            im=ax.scatter(YTemp[:,0][BCTTemp==c],YTemp[:,1][BCTTemp==c],\n",
        "                          c=SparsitiesTemp[BCTTemp==c]*-0.6, marker=markers[i],\n",
        "                          cmap=colourmaps[d], s=80)\n",
        "\n",
        "    ax.set_title(\"%s \" % (label),fontsize=15,fontweight=\"bold\")\n",
        "\n",
        "    ax.axis('tight')\n",
        "\n",
        "OrangePatch = mpatches.Patch(color='orange', label='Motion Regression')\n",
        "PurplePatch = mpatches.Patch(color='purple', label='Global Signal Regression')\n",
        "\n",
        "\n",
        "Lines={}\n",
        "for i, bct_model in enumerate(BCT_models):\n",
        "    Lines[i] = mlines.Line2D([], [], color='black', linestyle='None',\n",
        "                             marker=markers[i], markersize=10, \n",
        "                             label=bct_model)\n",
        "\n",
        "\n",
        "figDE.savefig(str(output_path / 'DifferentEmbeddings.png'),dpi=300)\n",
        "figDE.savefig(str(output_path / 'DifferentEmbeddings.svg'),format=\"svg\")\n",
        "figDE.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggt7wfAfkAiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Do the same as above but for MDS\n",
        "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=10, \n",
        "                              random_state=21, metric=True)\n",
        "Y = methods['MDS'].fit_transform(X)\n",
        "data_reduced['MDS'] = Y\n",
        "#Keep embedding space for subsequent use in active learning below\n",
        "ModelEmbedding = Y\n",
        "\n",
        "figMDS = plt.figure(constrained_layout=False, figsize=(21,15))\n",
        "gsMDS = figMDS.add_gridspec(nrows=15, ncols=20)\n",
        "ax = figMDS.add_subplot(gsMDS[:,0:15])\n",
        "\n",
        "for d in preprocessing:\n",
        "    BCTTemp=BCT[Data==d]\n",
        "    SparsitiesTemp=Sparsities[Data==d]\n",
        "    YTemp=Y[Data==d,:]\n",
        "\n",
        "    for i, c in enumerate(BCT_models):\n",
        "        im=ax.scatter(YTemp[:,0][BCTTemp==c],YTemp[:,1][BCTTemp==c],\n",
        "                      c=SparsitiesTemp[BCTTemp==c]*0.1, marker=markers[i],\n",
        "                      cmap=colourmaps[d], s=150)\n",
        "        ax.spines['top'].set_linewidth(1.5)\n",
        "        ax.spines['right'].set_linewidth(1.5)\n",
        "        ax.spines['bottom'].set_linewidth(1.5)\n",
        "        ax.spines['left'].set_linewidth(1.5)\n",
        "        ax.set_xlabel('Dimension 2',fontsize=20,fontweight=\"bold\")\n",
        "        ax.set_ylabel('Dimension 1',fontsize=20,fontweight=\"bold\")\n",
        "        ax.tick_params(labelsize=15)\n",
        "\n",
        "\n",
        "ax.set_title('Multi-dimensional Scaling', fontsize=25,fontweight=\"bold\")\n",
        "\n",
        "\n",
        "OrangePatch = mpatches.Patch(color='orange', label='motion regression')\n",
        "PurplePatch = mpatches.Patch(color=[85/255, 3/255, 152/255], label='global signal regression')\n",
        "\n",
        "IntensityPatch1 = mpatches.Patch(color=[0.1, 0.1, 0.1], label='threshold: 0.4',alpha=1)\n",
        "IntensityPatch2 = mpatches.Patch(color=[0.1, 0.1, 0.1], label='threshold: 0.1',alpha=0.4)\n",
        "IntensityPatch3 = mpatches.Patch(color=[0.1, 0.1, 0.1], label='threshold: 0.01',alpha=0.1)\n",
        "\n",
        "BlankLine=mlines.Line2D([], [], linestyle='None')\n",
        "\n",
        "Lines={}\n",
        "for i, bct_model in enumerate(BCT_models):\n",
        "    Lines[i] = mlines.Line2D([], [], color='black', linestyle='None',\n",
        "                             marker=markers[i],markersize=10, \n",
        "                             label=bct_model)\n",
        "\n",
        "figMDS.legend(handles=[OrangePatch, PurplePatch,BlankLine,IntensityPatch1,\n",
        "                       IntensityPatch2, IntensityPatch3,BlankLine,\n",
        "                       Lines[0],Lines[1],Lines[2],Lines[3],Lines[4],Lines[5],\n",
        "                       Lines[6],Lines[7],Lines[8],Lines[9],Lines[10],Lines[11],\n",
        "                       Lines[12],Lines[13],Lines[14],Lines[15]],fontsize=15,\n",
        "              frameon=False,bbox_to_anchor=(1.4, 0.8),bbox_transform=ax.transAxes)\n",
        "\n",
        " \n",
        "figMDS.savefig(str(output_path / 'MDSSpace.png'), dpi=300)\n",
        "figMDS.savefig(str(output_path /'MDSSpace.svg'), format=\"svg\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y19DU5UH_EWC",
        "colab_type": "text"
      },
      "source": [
        "## Analyse the neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x25diSb_IFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import kneighbors_graph\n",
        "\n",
        "def get_dissimilarity_n_neighbours(all_neighbours_orig,\n",
        "                                   all_neighbours_reduced):\n",
        "    '''\n",
        "    Calculate the dissimilarity\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    all_neighbours_orig:\n",
        "    all_neighbours_reduced:\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    all_dissimilarity: Dissimilarity scores between the original and reduced\n",
        "    space\n",
        "    '''\n",
        "\n",
        "    all_dissimilarity = []\n",
        "    for K in range(len(all_neighbours_reduced)):\n",
        "        # Find the set of different indices\n",
        "        diff = set(sorted(all_neighbours_orig[K])) - \\\n",
        "               set(sorted(all_neighbours_reduced[K]))\n",
        "        # Calculate the dissimilarity\n",
        "        epsilon = len(diff) / len(all_neighbours_orig[K])\n",
        "        all_dissimilarity.append(epsilon)\n",
        "\n",
        "    return all_dissimilarity\n",
        "\n",
        "def get_models_neighbours(N, n_neigbors_step, data):\n",
        "    '''\n",
        "    Calculate the dissimilarity\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_neighbours: number of neighbours to analyse\n",
        "    data: data (pairwise_subjects, n_analysis)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    all_dissimilarity: Dissimilarity scores between the original and reduced\n",
        "    space\n",
        "    '''\n",
        "    n_neighbours = range(2, N, n_neighbors_step)\n",
        "    all_adj = np.zeros((len(data), len(data), len(n_neighbours)))\n",
        "    all_neighbours_orig = []\n",
        "    \n",
        "    for idx, n_neighbour in enumerate(n_neighbours):\n",
        "        adj = kneighbors_graph(data, n_neighbour, mode='distance',\n",
        "                            metric='euclidean')\n",
        "        adj_array = adj.toarray()\n",
        "        all_adj[:, :, idx] = adj_array\n",
        "        nneighbours_orig = np.nonzero(adj_array)\n",
        "        nneighbours_orig = [item for item in zip(nneighbours_orig[0],\n",
        "                                                 nneighbours_orig[1])]\n",
        "        all_neighbours_orig.append(nneighbours_orig)\n",
        "\n",
        "    return all_neighbours_orig, all_adj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VOAQ8O518_cP",
        "colab": {}
      },
      "source": [
        "N = 544\n",
        "n_neighbors_step = 10\n",
        "\n",
        "neighbours_orig, adj_array = get_models_neighbours(N, n_neighbors_step, X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyfPmC_wNumB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neighbours_tsne, _ = get_models_neighbours(N, n_neighbors_step,\n",
        "                                           data_reduced['t-SNE'])\n",
        "diss_tsne = get_dissimilarity_n_neighbours(neighbours_orig, neighbours_tsne)\n",
        "del neighbours_tsne "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16BQTmevzoue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neighbours_lle, _ = get_models_neighbours(N, n_neighbors_step, \n",
        "                                          data_reduced['LLE'])\n",
        "diss_lle = get_dissimilarity_n_neighbours(neighbours_orig,neighbours_lle)\n",
        "del neighbours_lle "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phtJnBWCz1l6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neighbours_se, _ = get_models_neighbours(N, n_neighbors_step,\n",
        "                                         data_reduced['SE'])\n",
        "diss_se = get_dissimilarity_n_neighbours(neighbours_orig,neighbours_se)\n",
        "del neighbours_se"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r92E3fm3z6HC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neighbours_mds, _ = get_models_neighbours(N, n_neighbors_step,\n",
        "                                          data_reduced['MDS'])\n",
        "diss_mds = get_dissimilarity_n_neighbours(neighbours_orig,neighbours_mds)\n",
        "del neighbours_mds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaxUNFasas6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import hypergeom\n",
        "# Calculate the null distribution using binary distribution\n",
        "def expectation(N,K):\n",
        "    rv = hypergeom(N, K, K)\n",
        "    x = np.arange(0, K)\n",
        "    pmf = rv.pmf(x)\n",
        "    return np.sum(x*pmf)\n",
        "\n",
        "null_distribution = []\n",
        "for K in range(2, N, n_neighbors_step):\n",
        "    E = expectation(N,K)\n",
        "    diss = 1 - (E/K)\n",
        "    null_distribution.append(diss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDq5PDjwSgzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the same as the cell above, but use random values, instead of using\n",
        "# the binary formula. \n",
        "# Note: this is just a test to see if the above works well\n",
        "random_nn = []\n",
        "for idx in range(len(neighbours_orig)):\n",
        "  # Number of NN per points\n",
        "  K = int(len(neighbours_orig[idx])/N)\n",
        "  indices = np.zeros((K*N, 2))\n",
        "  indices[:, 0] = np.array([i[0] for i in neighbours_orig[idx]])\n",
        "  for jj in range(N):\n",
        "    random_n = rng.choice(N, size=K, replace=False)\n",
        "    indices[jj*K:(jj+1)*K, 1] = random_n\n",
        "  tmp = [(int(x[0]), int(x[1])) for x in indices] \n",
        "  random_nn.append(tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od_KIcl-bm_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the dissimilarity of the random distribution\n",
        "diss_random = get_dissimilarity_n_neighbours(neighbours_orig, random_nn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5XUNH1My_cW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "n_neighbours = range(2, N, n_neighbors_step)\n",
        "ax.plot(n_neighbours, diss_tsne, label='t-SNE', color='#1DACE8')\n",
        "ax.plot(n_neighbours, diss_lle, label='LLE', color='#E5C4A1')\n",
        "ax.plot(n_neighbours, diss_se, label='SE', color='#F24D29')\n",
        "ax.plot(n_neighbours, diss_mds, label='MDS', color='#1C366B')\n",
        "plt.plot(n_neighbours, null_distribution, label='random', c='grey')\n",
        "#plt.plot(n_neighbours, diss_random, label='random_rnd', c='k')\n",
        "plt.ylim([0,1])\n",
        "plt.xlim([0,N])\n",
        "plt.legend(frameon=False)\n",
        "plt.xlabel('$k$ Nearest Neighbors')\n",
        "plt.ylabel('Dissimilarity $\\epsilon_k$')\n",
        "plt.savefig(str(output_path / 'dissimilarity_all.svg'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4sodQ_wC4pY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download file to computer\n",
        "from google.colab import files\n",
        "files.download(str(output_path / 'dissimilarity_all.svg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Trmm0t50Ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "n_neighbours = range(2, N, n_neighbors_step)\n",
        "ax.plot(n_neighbours, np.array(diss_tsne)/np.array(null_distribution), label='t-SNE', color='#1DACE8')\n",
        "ax.plot(n_neighbours, np.array(diss_lle)/np.array(null_distribution), label='LLE', color='#E5C4A1')\n",
        "ax.plot(n_neighbours, np.array(diss_se)/np.array(null_distribution), label='SE', color='#F24D29')\n",
        "ax.plot(n_neighbours, np.array(diss_mds)/np.array(null_distribution), label='MDS', color='#1C366B')\n",
        "#plt.plot(n_neighbours, null_distribution, label='random', c='grey')\n",
        "#plt.plot(n_neighbours, diss_random, label='random_rnd', c='k')\n",
        "plt.ylim([0,1.2])\n",
        "plt.xlim([0,N])\n",
        "plt.legend(frameon=False)\n",
        "plt.xlabel('$k$ Nearest Neighbors')\n",
        "plt.ylabel('Dissimilarity $\\epsilon_k$')\n",
        "plt.savefig(str(output_path / 'dissimilarity_all_norm.svg'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8yM3HTB58LA",
        "colab_type": "text"
      },
      "source": [
        "## Exhaustive Search\n",
        "\n",
        "Exhaustive search for SVR prediction of age, so we know what \"ground truth\" is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QNBlUjI6Nwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "from bayes_opt import UtilityFunction\n",
        "from helperfunctions import objectiveFunc, bayesian_optimisation, display_gp_mean_uncertainty\n",
        "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib3ZKhDMmPcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PredictedAcc = np.zeros((len(Data_Run)))\n",
        "for i in range(len(Data_Run)):\n",
        "    tempPredAcc = objectiveFunc(i, AgesPrediction, Sparsities_Run, Data_Run,\n",
        "                              BCT_models, BCT_Run, KeptYeoIDs, MainNoNanPrediction,\n",
        "                              GSRNoNanPrediction, 1)\n",
        "    PredictedAcc[i] = tempPredAcc\n",
        "\n",
        "#Display how predicted accuracy is distributed across the low-dimensional space\n",
        "plt.scatter(ModelEmbedding[0: PredictedAcc.shape[0], 0],\n",
        "            ModelEmbedding[0: PredictedAcc.shape[0], 1],\n",
        "            c=PredictedAcc)\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aVnBYeLmQxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dump accuracies\n",
        "pickle.dump(PredictedAcc, open(str(output_path / 'predictedAcc.pckl'), 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfOFZKFyCiJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download file to computer\n",
        "from google.colab import files\n",
        "files.download(str(output_path / 'predictedAcc.pckl'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7UX0xYsKAdl",
        "colab_type": "text"
      },
      "source": [
        "## Active Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8aTNtvW3zp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import product\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "from bayes_opt import UtilityFunction\n",
        "from helperfunctions import objectiveFunc,bayesian_optimisation, display_gp_mean_uncertainty\n",
        "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import permutation_test_score\n",
        "from scipy.stats import spearmanr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l81tOZ9Z33WD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Helper function for calculating posterior predictions only for points in the space where an analysis approach exists \n",
        "def posteriorOnlyModels(gp, x_obs, y_obs, z_obs, AllModelEmb):\n",
        "    xy = (np.array([x_obs.ravel(), y_obs.ravel()])).T\n",
        "    gp.fit(xy, z_obs)\n",
        "    mu, std = gp.predict(AllModelEmb, return_std=True)\n",
        "    return mu, std, gp\n",
        "\n",
        "\n",
        "#Define the kernel: white noise kernel plus Mattern\n",
        "kernel = 1.0 * Matern(length_scale=25, length_scale_bounds=(10,80),nu=2.5) \\\n",
        "    + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-10, 0.1))\n",
        "\n",
        "\n",
        "\n",
        "lb1 = np.min(ModelEmbedding[:, 0])\n",
        "hb1 = np.max(ModelEmbedding[:, 0])\n",
        "lb2 = np.min(ModelEmbedding[:, 1])\n",
        "hb2 = np.max(ModelEmbedding[:, 1])\n",
        "pbounds = {'b1': (lb1, hb1), 'b2': (lb2, hb2)}\n",
        "\n",
        "\n",
        "\n",
        "#For finding nearest point in space to next suggested sample from Bayesian optimization\n",
        "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(ModelEmbedding)\n",
        "\n",
        "distances, indices = nbrs.kneighbors(ModelEmbedding)\n",
        "\n",
        "#Acquisition function, in this case upper confidence bound with exploratory kappa vaalue\n",
        "utility = UtilityFunction(kind=\"ucb\", kappa=10,xi=1e-1)\n",
        "\n",
        "init_points=10 #Number of burn in random initial samples\n",
        "n_iter=40 #Number of iterations of Bayesian optimization after burn in\n",
        "\n",
        "RandomSeed=118\n",
        "\n",
        "#Initialise optimizer    \n",
        "optimizer = BayesianOptimization(f=None,\n",
        "                                pbounds=pbounds,\n",
        "                                 verbose=4,\n",
        "                                 random_state=RandomSeed)\n",
        "\n",
        "optimizer.set_gp_params(kernel=kernel,normalize_y=True,n_restarts_optimizer=10)\n",
        "np.random.seed(RandomSeed)\n",
        "\n",
        "# Perform optimization. Given that the space is continuous and the analysis \n",
        "# approaches are not, we penalize suggestions that are far from any actual \n",
        "# analysis approaches. For these suggestions the registered value is set to the\n",
        "#  lowest value from the burn in. These points (FailedIters) are only used\n",
        "# during search but exluded when recalculating the GP regression after search.\n",
        "FailedIters=bayesian_optimisation(kernel, optimizer, utility, init_points,\n",
        "                                  n_iter, pbounds, nbrs,RandomSeed,\n",
        "                                  ModelEmbedding,BCT_models,BCT_Run,\n",
        "                                  Sparsities_Run,Data_Run,AgesPrediction,\n",
        "                                  KeptYeoIDs,MainNoNanPrediction,\n",
        "                                  GSRNoNanPrediction,1,-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSUlVAr3LQa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display the results of the active search and the evolution of the search\n",
        "# after 5, 10,20, 30 and 50 iterations.\n",
        "\n",
        "def _posterior(gp, x_obs, y_obs, z_obs, grid_X):\n",
        "    xy = (np.array([x_obs.ravel(), y_obs.ravel()])).T\n",
        "    gp.fit(xy, z_obs)\n",
        "    mu, std = gp.predict(grid_X.reshape(-1, 2), return_std=True)\n",
        "    return mu, std, gp\n",
        "\n",
        "def posteriorOnlyModels(gp, x_obs, y_obs, z_obs, AllModelEmb):\n",
        "    xy = (np.array([x_obs.ravel(), y_obs.ravel()])).T\n",
        "    gp.fit(xy, z_obs)\n",
        "    mu, std = gp.predict(AllModelEmb, return_std=True)\n",
        "    return mu, std, gp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "BadIter=FailedIters\n",
        "x = np.linspace(pbounds['b1'][0] - 10, pbounds['b1'][1] + 10, 500).reshape(\n",
        "    -1, 1)\n",
        "y = np.linspace(pbounds['b2'][0] - 10, pbounds['b2'][1] + 10, 500).reshape(\n",
        "    -1, 1)\n",
        "gp = GaussianProcessRegressor(kernel=kernel, normalize_y=True,\n",
        "                              n_restarts_optimizer=10)\n",
        "\n",
        "x_temp = np.array([[res[\"params\"][\"b1\"]] for res in optimizer.res])\n",
        "y_temp = np.array([[res[\"params\"][\"b2\"]] for res in optimizer.res])\n",
        "z_temp = np.array([res[\"target\"] for res in optimizer.res])\n",
        "\n",
        "x_obs=x_temp[BadIter==0]\n",
        "y_obs=y_temp[BadIter==0]\n",
        "z_obs=z_temp[BadIter==0]\n",
        "\n",
        "NumSamplesToInclude=x_obs.shape[0]\n",
        "x1x2 = np.array(list(product(x, y)))\n",
        "X0p, X1p = x1x2[:, 0].reshape(500, 500), x1x2[:, 1].reshape(500, 500)\n",
        "\n",
        "mu, sigma, gp = _posterior(gp, x_obs[0:NumSamplesToInclude], y_obs[0:NumSamplesToInclude], z_obs[0:NumSamplesToInclude], x1x2)\n",
        "\n",
        "Zmu = np.reshape(mu, (500, 500))\n",
        "Zsigma = np.reshape(sigma, (500, 500))\n",
        "\n",
        "conf0 = np.array(mu - 2 * sigma).reshape(500, 500)\n",
        "conf1 = np.array(mu + 2 * sigma).reshape(500, 500)\n",
        "\n",
        "X0p, X1p = np.meshgrid(x, y, indexing='ij')\n",
        "\n",
        "font_dict_title = {'fontsize': 25}\n",
        "font_dict_label = {'fontsize': 15}\n",
        "font_dict_label3 = {'fontsize': 15}\n",
        "print(x_obs.shape)\n",
        "vmax=Zmu.max()\n",
        "vmin=Zmu.min()\n",
        "\n",
        "\n",
        "cm = ['coolwarm', 'seismic']\n",
        "\n",
        "\n",
        "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(16,8))\n",
        "\n",
        "ax = ax1\n",
        "pcm = ax.pcolormesh(X0p, X1p, Zmu,vmax=vmax,vmin=vmin,cmap=cm[0],rasterized=True)  \n",
        "ax.set_xlim(-50, 50)\n",
        "ax.set_ylim(-50, 50)\n",
        "ax.set_aspect('equal', 'box')\n",
        "ax = ax2\n",
        "pcm = ax.scatter(ModelEmbedding[0:PredictedAcc.shape[0],0],ModelEmbedding[0:PredictedAcc.shape[0],1],c=PredictedAcc,vmax=vmax,vmin=vmin,cmap=cm[0],rasterized=True)\n",
        "ax.set_aspect('equal', 'box')\n",
        "\n",
        "fig.tight_layout()\n",
        "ax.set_xlim(-50, 50)\n",
        "ax.set_ylim(-50, 50)\n",
        "fig.subplots_adjust(right=0.8)\n",
        "cbar_ax = fig.add_axes([0.825, 0.35, 0.02, 0.3])\n",
        "\n",
        "fig.colorbar(pcm, cax=cbar_ax)\n",
        "\n",
        "\n",
        "fig.savefig(str(output_path / 'BOptAndTrueK01.png'),dpi=300)\n",
        "fig.savefig(str(output_path / 'BOptAndTrueK10.svg'),format='svg',dpi=300)\n",
        "\n",
        "\n",
        "\n",
        "font_dict_title = {'fontsize': 15}\n",
        "font_dict_label = {'fontsize': 10}\n",
        "font_dict_label3 = {'fontsize': 10}\n",
        "\n",
        "fig, axs = plt.subplots(5,3,figsize=(12,18))\n",
        "count=0\n",
        "for NumSamplesToInclude in [5,10,20,30,50]:\n",
        "\n",
        "    x1x2 = np.array(list(product(x, y)))\n",
        "    X0p, X1p = x1x2[:, 0].reshape(500, 500), x1x2[:, 1].reshape(500, 500)\n",
        "    mu, sigma, gp = _posterior(gp, x_obs[0:NumSamplesToInclude], y_obs[0:NumSamplesToInclude], z_obs[0:NumSamplesToInclude], x1x2)\n",
        "    muModEmb,sigmaModEmb,gpModEmb=posteriorOnlyModels(gp, x_obs[0:NumSamplesToInclude], y_obs[0:NumSamplesToInclude], z_obs[0:NumSamplesToInclude],ModelEmbedding)\n",
        "    Zmu = np.reshape(mu, (500, 500))\n",
        "    Zsigma = np.reshape(sigma, (500, 500))\n",
        "\n",
        "    conf0 = np.array(mu - 2 * sigma).reshape(500, 500)\n",
        "    conf1 = np.array(mu + 2 * sigma).reshape(500, 500)\n",
        "\n",
        "    X0p, X1p = np.meshgrid(x, y, indexing='ij')\n",
        "\n",
        "    ax = axs[count,0]\n",
        "\n",
        "    pcm = ax.pcolormesh(X0p, X1p, Zmu,vmax=vmax,vmin=vmin,cmap=cm[0],rasterized=True)\n",
        "    ax.set_aspect('equal', 'box')\n",
        "    ax.set_xlim(-50, 50)\n",
        "    ax.set_ylim(-50, 50)\n",
        "    ax = axs[count,1]\n",
        "\n",
        "    pcm = ax.pcolormesh(X0p, X1p, Zsigma,cmap=cm[1],rasterized=True)#,vmax=vmax,vmin=vmin)    \n",
        "    ax.set_title(\"Iterations: %i\" % (NumSamplesToInclude),fontsize=15,fontweight=\"bold\")\n",
        "    ax.set_aspect('equal', 'box')\n",
        "    ax.set_xlim(-50, 50)\n",
        "    ax.set_ylim(-50, 50)\n",
        "\n",
        "    ax = axs[count,2]\n",
        "    pcm=ax.scatter(muModEmb[PredictedAcc!=PredictedAcc.min()],PredictedAcc[PredictedAcc!=PredictedAcc.min()],marker='.',c='gray',rasterized=True)\n",
        "    ax.set_xlim(-0.225, -0.265)\n",
        "    ax.set_ylim(-0.225, -0.265)\n",
        "    ax.set_aspect('equal', 'box')\n",
        "\n",
        "    count=count+1\n",
        "\n",
        "\n",
        "\n",
        "fig.savefig(str(output_path / 'BOptEvolutionK10.svg'),format='svg',dpi=300)\n",
        "\n",
        "\n",
        "#Calculate the correlation between the actual and predicted space\n",
        "\n",
        "print(np.corrcoef(muModEmb,PredictedAcc))\n",
        "print(spearmanr(muModEmb,PredictedAcc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMD8DNNKLxNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel = 1.0 * Matern(length_scale=25, length_scale_bounds=(10,80),nu=2.5) \\\n",
        "    + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-10, 0.1))\n",
        "\n",
        "lb1 = np.min(ModelEmbedding[:, 0])\n",
        "hb1 = np.max(ModelEmbedding[:, 0])\n",
        "lb2 = np.min(ModelEmbedding[:, 1])\n",
        "hb2 = np.max(ModelEmbedding[:, 1])\n",
        "pbounds = {'b1': (lb1, hb1), 'b2': (lb2, hb2)}\n",
        "\n",
        "n_repetitions = 20\n",
        "BestModelGPSpace=np.zeros(n_repetitions)\n",
        "BestModelGPSpaceModIndex=np.zeros(n_repetitions)\n",
        "BestModelEmpirical=np.zeros(n_repetitions)\n",
        "BestModelEmpiricalModIndex=np.zeros(n_repetitions)\n",
        "ModelActualAccuracyCorrelation=np.zeros(n_repetitions)\n",
        "CVPValBestModels=np.zeros(n_repetitions)\n",
        "\n",
        "for DiffInit in range(n_repetitions):\n",
        "    optimizer = BayesianOptimization(f=None,\n",
        "                                     pbounds=pbounds,\n",
        "                                     verbose=4,\n",
        "                                     random_state=166+DiffInit)\n",
        "\n",
        "    optimizer.set_gp_params(kernel=kernel,normalize_y=True,n_restarts_optimizer=10)\n",
        "\n",
        "    nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(ModelEmbedding)\n",
        "\n",
        "    distances, indices = nbrs.kneighbors(ModelEmbedding)\n",
        "\n",
        "    utility = UtilityFunction(kind=\"ucb\", kappa=10,xi=1e-1)\n",
        "\n",
        "\n",
        "    n_iter=10\n",
        "    init_points=10\n",
        "    RandomSeed=111+DiffInit\n",
        "    np.random.seed(RandomSeed)\n",
        "    FailedIters=bayesian_optimisation(kernel, optimizer, utility, init_points,\n",
        "                                      n_iter, pbounds, nbrs,RandomSeed,\n",
        "                                      ModelEmbedding,BCT_models,BCT_Run,\n",
        "                                      Sparsities_Run,Data_Run,AgesPrediction,\n",
        "                                      KeptYeoIDs,MainNoNanPrediction,\n",
        "                                      GSRNoNanPrediction,1,-1)\n",
        "    \n",
        "    gp = GaussianProcessRegressor(kernel=kernel, normalize_y=True,\n",
        "                                  n_restarts_optimizer=10)\n",
        "\n",
        "    x_temp = np.array([[res[\"params\"][\"b1\"]] for res in optimizer.res])\n",
        "    y_temp = np.array([[res[\"params\"][\"b2\"]] for res in optimizer.res])\n",
        "    z_temp = np.array([res[\"target\"] for res in optimizer.res])\n",
        "\n",
        "    x_obs=x_temp[FailedIters==0]\n",
        "    y_obs=y_temp[FailedIters==0]\n",
        "    z_obs=z_temp[FailedIters==0]\n",
        "    \n",
        "    muModEmb,sigmaModEmb,gpModEmb=posteriorOnlyModels(gp, x_obs, y_obs, z_obs,ModelEmbedding)\n",
        "    \n",
        "    BestModelGPSpace[DiffInit]=muModEmb.max()\n",
        "    BestModelGPSpaceModIndex[DiffInit]=muModEmb.argmax()\n",
        "    BestModelEmpirical[DiffInit]=z_obs.max()\n",
        "    Model_coord = np.array([[x_obs[z_obs.argmax()][-1], y_obs[z_obs.argmax()][-1]]])\n",
        "    BestModelEmpiricalModIndex[DiffInit]=nbrs.kneighbors(Model_coord)[1][0][0]\n",
        "    ModelActualAccuracyCorrelation[DiffInit]=spearmanr(muModEmb,PredictedAcc)[0]\n",
        "    \n",
        "    ClassOrRegress=1\n",
        "    TempModelNum=muModEmb.argmax()\n",
        "    Y=AgesPrediction\n",
        "    CommunityIDs=KeptYeoIDs\n",
        "    if Data_Run[TempModelNum]==0:\n",
        "        TempData=MainNoNanPrediction # BUG BUG BUG \n",
        "        TotalRegions=346\n",
        "        TotalSubjects=TempData.shape[2]\n",
        "    elif Data_Run[TempModelNum]==1:\n",
        "        TempData=GSRNoNanPrediction\n",
        "        TotalRegions=346\n",
        "        TotalSubjects=TempData.shape[2]\n",
        "    \n",
        "    \n",
        "    \n",
        "    TempThreshold=Sparsities_Run[TempModelNum]\n",
        "    \n",
        "    BCT_Num=[i for i, e in enumerate(BCT_models) if e[0] == BCT_Run[TempModelNum]][0]\n",
        "    \n",
        "    TempResults=np.zeros([TotalSubjects, n_regions])\n",
        "    for SubNum in range(0,TotalSubjects):\n",
        "        x = bct.threshold_proportional(TempData[:,:,SubNum], TempThreshold, copy=True)\n",
        "        if BCT_Num==7:\n",
        "            s=np.asarray(BCT_models[BCT_Num][1](x,1))\n",
        "        elif BCT_Num==8:\n",
        "            temp_s=np.asarray(BCT_models[BCT_Num][1](x))\n",
        "            s=temp_s[0]\n",
        "        elif BCT_Num==9:\n",
        "            temp_s=np.asarray(BCT_models[BCT_Num][1](x))\n",
        "            s=temp_s[0]\n",
        "        elif BCT_Num==10:\n",
        "            s=np.asarray(BCT_models[BCT_Num][1](x,CommunityIDs))\n",
        "        elif BCT_Num==11:\n",
        "            s=np.asarray(BCT_models[BCT_Num][1](x,CommunityIDs))\n",
        "        elif BCT_Num==12:\n",
        "            s=np.asarray(BCT_models[BCT_Num][1](x,0.85))\n",
        "                #elif BCT_Num==13:\n",
        "                #   temp_s=np.asarray(BCT_models[BCT_Num][1](x))\n",
        "                #   s=temp_s[0]\n",
        "        elif BCT_Num==13:\n",
        "                #temp_s=np.asarray(BCT_models[BCT_Num][1](x,KeptYeoIDs,'degree'))\n",
        "            temp_s=np.asarray(BCT_models[BCT_Num][1](x,CommunityIDs))\n",
        "            s=temp_s[0] \n",
        "        elif BCT_Num==14:\n",
        "            temp_s=np.asarray(BCT_models[BCT_Num][1](x,CommunityIDs))\n",
        "            s=temp_s[0] \n",
        "        elif BCT_Num==15:\n",
        "            temp_s=np.asarray(BCT_models[BCT_Num][1](x))\n",
        "            s=temp_s[0]\n",
        "        else:\n",
        "            s=np.asarray(BCT_models[BCT_Num][1](x))\n",
        "\n",
        "        TempResults[SubNum,:]=s \n",
        "    scaler = StandardScaler()\n",
        "    TempResults=scaler.fit_transform(TempResults)\n",
        "  \n",
        "    model = SVR(C=1.0, epsilon=0.2)\n",
        "    #rs = np.random.RandomState(100)\n",
        "    TempScore=permutation_test_score(model, TempResults, AgesPrediction.ravel(),\n",
        "                                     groups=None, cv=None, n_permutations=5000, \n",
        "                                     n_jobs=None, random_state=5, verbose=0,\n",
        "                                     scoring=\"neg_mean_absolute_error\")\n",
        "    CVPValBestModels[DiffInit]=TempScore[2]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_0lraz1fCt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TEMP\n",
        "results = {}\n",
        "results['ModelEmbedding'] = ModelEmbedding\n",
        "results['BestModelGPSpaceModIndex'] = BestModelGPSpaceModIndex\n",
        "results['BestModelEmpiricalModIndex'] = BestModelEmpiricalModIndex\n",
        "results['BestModelEmpirical'] = BestModelEmpirical\n",
        "results['ModelActualAccuracyCorrelation'] = ModelActualAccuracyCorrelation\n",
        "results['TempResults'] = TempResults\n",
        "pickle.dump(results, open(str(output_path / 'results.pckl'), 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMqeSsZlL28V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# displaying results of 20 iterations\n",
        "\n",
        "fig8 = plt.figure(constrained_layout=False,figsize=(18,6))\n",
        "gs1 = fig8.add_gridspec(nrows=6, ncols=18)\n",
        "ax1 = fig8.add_subplot(gs1[:,0:6])\n",
        "ax1.set_title('Optima GP regression: 20 iterations',fontsize=15,fontweight=\"bold\")\n",
        "ax1.scatter(ModelEmbedding[0:PredictedAcc.shape[0],0],\n",
        "            ModelEmbedding[0:PredictedAcc.shape[0],1],\n",
        "            c=PredictedAcc,vmax=vmax,vmin=vmin,cmap='coolwarm',alpha=0.2,s=120)\n",
        "ax1.scatter(ModelEmbedding[BestModelGPSpaceModIndex.astype(int)][:,0],\n",
        "            ModelEmbedding[BestModelGPSpaceModIndex.astype(int)][:,1],s=120,c='black')\n",
        "\n",
        "ax1.set_xlim(-50, 50)\n",
        "ax1.set_ylim(-50, 50)\n",
        "\n",
        "ax2 = fig8.add_subplot(gs1[:,7:13])\n",
        "ax2.set_title('Empirical optima: 20 iterations',fontsize=15,fontweight=\"bold\")\n",
        "ax2.scatter(ModelEmbedding[0:PredictedAcc.shape[0],0],\n",
        "            ModelEmbedding[0:PredictedAcc.shape[0],1],\n",
        "            c=PredictedAcc,vmax=vmax,vmin=vmin,cmap='coolwarm',s=120,alpha=0.2)\n",
        "ax2.scatter(ModelEmbedding[BestModelEmpiricalModIndex.astype(int)][:,0],\n",
        "            ModelEmbedding[BestModelEmpiricalModIndex.astype(int)][:,1],c='black',s=120)\n",
        "\n",
        "ax2.set_xlim(-50, 50)\n",
        "ax2.set_ylim(-50, 50)\n",
        "\n",
        "ax3 = fig8.add_subplot(gs1[:,14:16])\n",
        "ax3.violinplot([PredictedAcc,BestModelEmpirical])\n",
        "ax3.set_xticks([1, 2])\n",
        "ax3.set_xticklabels(['Accuracy \\n of all points', 'Accuracy\\n of optima'],fontsize=9)\n",
        "\n",
        "ax4 = fig8.add_subplot(gs1[:,17:18])\n",
        "ax4.violinplot([ModelActualAccuracyCorrelation])\n",
        "ax4.set_xticks([1])\n",
        "ax4.set_xticklabels(['Correlation: \\n est vs emp '],fontsize=9)\n",
        "\n",
        "gs1\n",
        "fig8.savefig(str(output_path / 'BOpt20Repeats.png'),dpi=300) \n",
        "fig8.savefig(str(output_path / 'BOpt20Repeats.svg'),format=\"svg\") "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}